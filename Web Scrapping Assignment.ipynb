{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Web Scrapping Assignment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Web scraping (or data scraping) is a technique used to collect content and data from the internet. This data is usually saved in a local file so that it can be manipulated and analyzed as needed. If you’ve ever copied and pasted content from a website into an Excel spreadsheet, this is essentially what web scraping is, but on a very small scale.\n",
    "\n",
    "However, when people refer to ‘web scrapers,’ they’re usually talking about software applications. Web scraping applications (or ‘bots’) are programmed to visit websites, grab the relevant pages and extract useful information. By automating this process, these bots can extract huge amounts of data in a very short time. This has obvious benefits in the digital age, when big data—which is constantly updating and changing—plays such a prominent role.\n",
    "\n",
    "Web scraping has countless applications, especially within the field of data analytics. Market research companies use scrapers to pull data from social media or online forums for things like customer sentiment analysis. Others scrape data from product sites like Amazon or eBay to support competitor analysis.\n",
    "\n",
    "Meanwhile, Google regularly uses web scraping to analyze, rank, and index their content. Web scraping also allows them to extract information from third-party websites before redirecting it to their own (for instance, they scrape e-commerce sites to populate Google Shopping).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is web scraping used for?\n",
    "\n",
    "#### 1) Price intelligence\n",
    "In our experience, price intelligence is the biggest use case for web scraping.\n",
    "Extracting product and pricing information from e-commerce websites, then turning it into intelligence is an important part of modern e-commerce companies that want to make better pricing/marketing decisions based on data.\n",
    "\n",
    "Web pricing data and price intelligence benefits:\n",
    "\n",
    "Dynamic pricing\n",
    "Revenue optimization\n",
    "Competitor monitoring\n",
    "Product trend monitoring\n",
    "Brand and MAP compliance\n",
    "\n",
    "#### 2) Market research\n",
    "Market research is critical – and should be driven by the most accurate information available. With data scraping, you get high quality, high volume, and highly insightful web scraped data of every shape and size is fueling market analysis and business intelligence across the globe.\n",
    "\n",
    "Market trend analysis\n",
    "Market pricing\n",
    "Optimizing point of entry\n",
    "Research & development\n",
    "Competitor monitoring\n",
    "\n",
    "#### 3) Alternative data for finance\n",
    "Unearth alpha and radically create value with web data tailored specifically for investors.\n",
    "\n",
    "The decision-making process has never been as informed, nor data as insightful – and the world’s leading firms are increasingly consuming web scraped data, given its incredible strategic value.\n",
    "\n",
    "Extracting Insights from SEC Filings\n",
    "Estimating Company Fundamentals\n",
    "Public Sentiment Integrations\n",
    "News Monitoring\n",
    "\n",
    "#### 4) Real estate\n",
    "The digital transformation of real estate in the past twenty years threatens to disrupt traditional firms and create powerful new players in the industry.\n",
    "\n",
    "By incorporating web scraped product data into everyday business, agents and brokerages can protect against top-down online competition and make informed decisions within the market.\n",
    "\n",
    "Appraising Property Value\n",
    "Monitoring Vacancy Rates\n",
    "Estimating Rental Yields\n",
    "Understanding Market Direction\n",
    "\n",
    "#### 5) News & content monitoring\n",
    "Modern media can create outstanding value or an existential threat to your business - in a single news cycle.\n",
    "\n",
    "If you’re a company that depends on timely news analyses, or a company that frequently appears in the news, web scraping news data is the ultimate solution for monitoring, aggregating, and parsing the most critical stories from your industry.\n",
    "\n",
    "Investment Decision Making\n",
    "Online Public Sentiment Analysis\n",
    "Competitor Monitoring\n",
    "Political Campaigns\n",
    "Sentiment Analysis\n",
    "Lead generation\n",
    "Lead generation is a crucial marketing/sales activity for all businesses.\n",
    "\n",
    "In the 2020 Hubspot report, 61% of inbound marketers said generating traffic and leads was their number 1 challenge. Fortunately, web data extraction can be used to get access to structured lead lists from the web."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. What are the different methods used for Web Scraping?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are a few techniques commonly used to scrape data from websites. In general, all web scraping techniques retrieve content from websites, process it using a scraping engine, and generate one or more data files with the extracted content.\n",
    "\n",
    "#### 1) HTML Parsing\n",
    "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "#### 2) DOM Parsing\n",
    "The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
    "\n",
    "#### 3) Vertical Aggregation\n",
    "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
    "\n",
    "#### 4) XPath\n",
    "XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "#### 5) Google Sheets\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check if a website can be scraped or is protected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Beautiful Soup\n",
    "Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "\n",
    "#### Uses of Beautiful Soup\n",
    "The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in the document with which we’re working.\n",
    "\n",
    "#### Features of Beautiful Soup\n",
    "##### Some key features that make beautiful soup unique are:\n",
    "\n",
    "Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\n",
    "Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, which allows​ us to try out different parsing strategies or trade speed for flexibility."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape.\n",
    "\n",
    "##### In your file, type the following code:\n",
    "\n",
    "from flask import Flask, render_template\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "The first line imports the Flask class and the render_template method from the flask library. The second line imports the BeautifulSoup class, and the third line imports the requests module from our Python library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Aws services used in this projects are code pipeline and elastic beanstalk:-\n",
    "#### AWS CodePipeline\n",
    "AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.\n",
    "\n",
    "You can use CodePipeline to help you automatically build, test, and deploy your applications in the cloud. Specifically, you can:\n",
    "\n",
    "##### Automate your release processes:\n",
    "CodePipeline fully automates your release process from end to end, starting from your source repository through build, test, and deployment. You can prevent changes from moving through a pipeline by including a manual approval action in any stage except a Source stage. You can release when you want, in the way you want, on the systems of your choice, across one instance or multiple instances.\n",
    "\n",
    "##### Establish a consistent release process:\n",
    "Define a consistent set of steps for every code change. CodePipeline runs each stage of your release according to your criteria.\n",
    "\n",
    "##### Speed up delivery while improving quality:\n",
    "You can automate your release process to allow your developers to test and release code incrementally and speed up the release of new features to your customers.\n",
    "\n",
    "##### Use your favorite tools:\n",
    "You can incorporate your existing source, build, and deployment tools into your pipeline. For a full list of AWS services and third-party tools currently supported by CodePipeline\n",
    "\n",
    "##### View progress at a glance:\n",
    "You can review real-time status of your pipelines, check the details of any alerts, retry failed actions, view details about the source revisions used in the latest pipeline execution in each stage, and manually rerun any pipeline.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### AWS Elastic Beanstalk\n",
    "With AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
